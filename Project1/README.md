## PROJECT 1
### MODELING DB WITH POSTGRES

**Sparkify is a music streaming startup , they have been collecting data about user activity on their system.**
**Sparkify analytics team is trying to understand more about what users are listening to**

**Postgres database has been designed with tables to optimize queries on song play analysis**
**Helps share some light on what users are listening to**

**For example what userAgent is being used to access the platform & Analyze number of paid users and number of free users which could help target the free users to subscribing**

EX Query:
``` sql
    SELECT Count(*) agent_count, user_agent FROM SongPlays
    GROUP BY user_agent
    ORDER BY 1 DESC
    
```


EX Query:
``` sql
    SELECT Count(*) level_count, level FROM SongPlays
    GROUP BY level
    ORDER BY 1 DESC
    
```

Fact & Dimensions Table:

![alt text](https://www.lucidchart.com/publicSegments/view/18f92e71-d445-47cd-bf78-1dc7b658a0d1/image.png)


#### DATASETS:
* Subset of [Million Song Dataset](http://millionsongdataset.com/)
* Logdata generated by an [Event Log Simulator](https://github.com/Interana/eventsim)

#### Files included in the REPO:
- data folder which contains 2 folders :
1. log_data: consists of files in JSON format generated by an event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations, they are partitioned by year and month
EX: log_data/2018/11/2018-11-12-events.json
2. song_data: consists of files in JSON fromat contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
EX: song_data/A/B/C/TRABCEI128F424C983.json
- etl.ipynb jupyter notebook which contains detailed steps of the ETL Process used to process the files and insert the data in the fact and dimensions tables
- test.ipynb jupyter notebook which have some test SELECT statements to show the data being successfully inserted to the table
- sql_queries python script that contains the DB create & insert statements sepearted in their own file for modularity 
- create_tables.py python file that automatically drops the tables if they already exists and creates the tables as defined in the SQL Queries
- etl python script that contains the main program and manages the file processing needed for reading the files in JSON formats and inserting the data to the DB tables that was defined by the create_tables.py script
#### STEPS TO RUN PROJECT:
1. RUN create_tables.py file to create all DB tables
2. RUN etl.py file to insert all records in the tables

